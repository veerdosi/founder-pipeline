{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "J8FEoyBf_sQ9",
    "outputId": "6bb1e70a-08ec-4a64-9636-26cb141fbd77"
   },
   "outputs": [],
   "source": [
    "!pip install openai google_search_results serpapi python-dotenv apify aiohttp nest-asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "import requests\n",
    "import csv\n",
    "import os\n",
    "import aiohttp\n",
    "import statistics\n",
    "from datetime import datetime\n",
    "from pydantic import BaseModel\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from serpapi import GoogleSearch\n",
    "from typing import List, Dict, Any, Optional\n",
    "from urllib.parse import urlparse\n",
    "from apify_client import ApifyClient\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "# Set environment variables\n",
    "SERPER_API_KEY = os.getenv(\"SERPER_API_KEY\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "CRUNCHBASE_API_KEY = os.getenv(\"CRUNCHBASE_API_KEY\")\n",
    "PERPLEXITY_API_KEY = os.getenv(\"PERPLEXITY_API_KEY\")\n",
    "APIFY_TOKEN = os.getenv (\"APIFY_API_KEY\")\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Profile(BaseModel):\n",
    "    name: str\n",
    "    url: str\n",
    "\n",
    "class ProfileList(BaseModel):\n",
    "    linkedin: List[Profile]\n",
    "\n",
    "class SearchQueries(BaseModel):\n",
    "    queries: List[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "COF-JzKq8N5Z",
    "outputId": "81c09a5d-4835-49f4-ffa8-d3ec2a470f63"
   },
   "outputs": [],
   "source": [
    "def search_crunchbase_companies():\n",
    "    \"\"\"Search for AI companies in Crunchbase\"\"\"\n",
    "    \n",
    "    # API endpoint\n",
    "    SEARCH_URL = f\"https://api.crunchbase.com/v4/data/searches/organizations?user_key={CRUNCHBASE_API_KEY}\"\n",
    "    \n",
    "    # UUIDs for filters\n",
    "    location_uuid = \"f110fca2-1055-99f6-996d-011c198b3928\"  # United States\n",
    "    category_uuids = [\n",
    "        \"c4d8caf3-5fe7-359b-f9f2-2d708378e4ee\",  # artificial-intelligence\n",
    "        \"186d333a-99df-4a4a-6a0f-69bd2c0d0bba\",  # intelligent-systems\n",
    "        \"5ea0cdb7-c9a6-47fc-50f8-c9b0fac04863\"   # machine-learning\n",
    "    ]\n",
    "    \n",
    "    # Request payload\n",
    "    payload = {\n",
    "        \"field_ids\": [\n",
    "            \"identifier\", \"categories\", \"location_identifiers\", \"short_description\",\n",
    "            \"description\", \"funding_total\", \"founder_identifiers\", \"founded_on\",\n",
    "            \"linkedin\", \"investor_identifiers\", \"last_equity_funding_type\", \"num_founders\"\n",
    "        ],\n",
    "        \"query\": [\n",
    "            {\n",
    "                \"type\": \"predicate\",\n",
    "                \"field_id\": \"categories\",\n",
    "                \"operator_id\": \"includes\",\n",
    "                \"values\": category_uuids\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"predicate\",\n",
    "                \"field_id\": \"location_identifiers\",\n",
    "                \"operator_id\": \"includes\",\n",
    "                \"values\": [location_uuid]\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"predicate\",\n",
    "                \"field_id\": \"founded_on\",\n",
    "                \"operator_id\": \"gte\",\n",
    "                \"values\": [\"2010-01-01\"]\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"predicate\",\n",
    "                \"field_id\": \"funding_total\",\n",
    "                \"operator_id\": \"gte\",\n",
    "                \"values\": [{\"currency\": \"USD\", \"value\": 100000}]\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"predicate\",\n",
    "                \"field_id\": \"facet_ids\",\n",
    "                \"operator_id\": \"includes\",\n",
    "                \"values\": [\"company\"]\n",
    "            }\n",
    "        ],\n",
    "        \"limit\": 20,\n",
    "    }\n",
    "    \n",
    "    # Make the POST request\n",
    "    response = requests.post(SEARCH_URL, json=payload)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        results = response.json().get(\"entities\", [])\n",
    "        output_rows = []\n",
    "        \n",
    "        for item in results:\n",
    "            props = item.get(\"properties\", {})\n",
    "            identifier = props.get(\"identifier\", {})\n",
    "            \n",
    "            # Extract company data\n",
    "            name = identifier.get(\"value\", \"\")\n",
    "            uuid = item.get(\"uuid\", \"\")\n",
    "            linkedin = props.get(\"linkedin\", {}).get(\"value\", \"\")\n",
    "            short_description = props.get(\"short_description\", \"\")\n",
    "            full_description = props.get(\"description\", \"\")\n",
    "            created_at = props.get(\"founded_on\", {}).get(\"value\", \"\")\n",
    "            \n",
    "            # Founders\n",
    "            founder_list = props.get(\"founder_identifiers\", [])\n",
    "            founders = \"|\".join([founder.get(\"value\", \"\") for founder in founder_list])\n",
    "            \n",
    "            # Investors\n",
    "            investor_list = props.get(\"investor_identifiers\", [])\n",
    "            investors = \"|\".join([inv.get(\"value\", \"\") for inv in investor_list])\n",
    "            \n",
    "            # Categories\n",
    "            categories = props.get(\"categories\", [])\n",
    "            category_list = \"|\".join([cat.get(\"value\", \"\") for cat in categories])\n",
    "            \n",
    "            # Locations\n",
    "            locations = props.get(\"location_identifiers\", [])\n",
    "            loc_dict = {loc[\"location_type\"]: loc.get(\"value\", \"\") for loc in locations}\n",
    "            city = loc_dict.get(\"city\", \"\")\n",
    "            region = loc_dict.get(\"region\", \"\")\n",
    "            country = loc_dict.get(\"country\", \"\")\n",
    "            continent = loc_dict.get(\"continent\", \"\")\n",
    "            \n",
    "            # Funding\n",
    "            funding_total = props.get(\"funding_total\", {}).get(\"value_usd\", \"\")\n",
    "            last_funding_type = props.get(\"last_equity_funding_type\", \"\")\n",
    "            \n",
    "            # Append row\n",
    "            output_rows.append({\n",
    "                \"uuid\": uuid,\n",
    "                \"name\": name,\n",
    "                \"linkedin\": linkedin,\n",
    "                \"short_description\": short_description,\n",
    "                \"full_description\": full_description,\n",
    "                \"founded_on\": created_at,\n",
    "                \"founders\": founders,\n",
    "                \"investors\": investors,\n",
    "                \"categories\": category_list,\n",
    "                \"city\": city,\n",
    "                \"region\": region,\n",
    "                \"country\": country,\n",
    "                \"continent\": continent,\n",
    "                \"funding_total_usd\": funding_total,\n",
    "                \"last_funding_type\": last_funding_type\n",
    "            })\n",
    "        \n",
    "        # Save to CSV\n",
    "        df = pd.DataFrame(output_rows)\n",
    "        df.to_csv(\"company_data.csv\", index=False)\n",
    "        print(f\"✅ Results saved to 'company_data.csv' - {len(output_rows)} companies found\")\n",
    "        return df\n",
    "    else:\n",
    "        print(f\"❌ Error {response.status_code}: {response.text}\")\n",
    "        return None\n",
    "\n",
    "# Run the search\n",
    "company_df = search_crunchbase_companies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-i-LztI__zxW",
    "outputId": "b8a42026-8631-47b1-87c7-cf108b1c654b"
   },
   "outputs": [],
   "source": [
    "def extract_name_from_url(url: str) -> str:\n",
    "    \"\"\"Extract name from LinkedIn URL\"\"\"\n",
    "    parts = url.split('/')\n",
    "    if 'in' in parts:\n",
    "        name_part = parts[parts.index('in') + 1].replace('-', ' ').replace('%20', ' ')\n",
    "        return name_part.title()\n",
    "    return \"Unknown\"\n",
    "\n",
    "def search_google(query: str, n: int = 10) -> List[Dict[str, str]]:\n",
    "    \"\"\"Search Google for LinkedIn profiles\"\"\"\n",
    "    params = {\n",
    "        \"engine\": \"google\",\n",
    "        \"q\": query,\n",
    "        \"api_key\": SERPER_API_KEY,\n",
    "        \"num\": n\n",
    "    }\n",
    "    \n",
    "    search = GoogleSearch(params)\n",
    "    results = search.get_dict()\n",
    "    organic_results = results.get(\"organic_results\", [])\n",
    "    \n",
    "    return [\n",
    "        {\n",
    "            \"title\": r.get(\"title\", \"\"),\n",
    "            \"link\": r.get(\"link\", \"\"),\n",
    "            \"snippet\": r.get(\"snippet\", \"\")\n",
    "        }\n",
    "        for r in organic_results\n",
    "        if \"linkedin.com/in/\" in r.get(\"link\", \"\")\n",
    "    ]\n",
    "\n",
    "def generate_search_queries(company: str, description: str, titles: List[str]) -> List[str]:\n",
    "    \"\"\"Generate optimized search queries for finding LinkedIn profiles\"\"\"\n",
    "    prompt = f\"\"\"\n",
    "You are a smart assistant designed to generate Google search queries to find LinkedIn profiles of specific roles at a company.\n",
    "\n",
    "--- INPUT ---\n",
    "Company Name: \"{company}\"\n",
    "Company Description: \"{description}\"\n",
    "Titles of interest: {', '.join(titles)}\n",
    "\n",
    "--- OBJECTIVE ---\n",
    "Create as few high-quality search queries as possible that:\n",
    "1. Are optimized to find LinkedIn profiles of people holding the target roles at this exact company.\n",
    "2. Use advanced search operators like site:linkedin.com/in\n",
    "\"\"\"\n",
    "    \n",
    "    response = client.beta.chat.completions.parse(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        response_format=SearchQueries,\n",
    "        temperature=0.0\n",
    "    )\n",
    "    return response.choices[0].message.parsed.queries\n",
    "\n",
    "def extract_profiles_from_search(results: List[Dict[str, str]], company: str, description: str, titles: List[str]) -> List[Profile]:\n",
    "    \"\"\"Extract relevant profiles from search results\"\"\"\n",
    "    combined = \"\\n\".join(f\"{r['title']} - {r['link']}\\n{r['snippet']}\" for r in results)\n",
    "    prompt = f\"\"\"\n",
    "You are a filtering assistant helping identify relevant LinkedIn profiles.\n",
    "\n",
    "Company Name: {company}\n",
    "Company Description: {description}\n",
    "Roles of interest: {', '.join(titles)}\n",
    "\n",
    "Below are search results from Google (including LinkedIn links). Your task is to extract only those LinkedIn profiles that are likely to belong to employees of this company in leadership roles like the ones mentioned.\n",
    "\n",
    "Search Results:\n",
    "{combined}\n",
    "\"\"\"\n",
    "    \n",
    "    response = client.beta.chat.completions.parse(\n",
    "        model=\"o3-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        response_format=ProfileList\n",
    "    )\n",
    "    return response.choices[0].message.parsed.linkedin\n",
    "\n",
    "def extract_profiles_from_search_with_names(results: List[Dict[str, str]], company: str, description: str, titles: List[str], known_names: List[str]) -> List[Profile]:\n",
    "    \"\"\"Extract profiles using known founder names\"\"\"\n",
    "    combined = \"\\n\".join(f\"{r['title']} - {r['link']}\\n{r['snippet']}\" for r in results)\n",
    "    prompt = f\"\"\"\n",
    "You are a filtering assistant helping identify LinkedIn profiles.\n",
    "\n",
    "Company Name: {company}\n",
    "Company Description: {description}\n",
    "Known founder names: {', '.join(known_names)}\n",
    "Roles of interest: {', '.join(titles)}\n",
    "\n",
    "Below are search results from Google. Extract only those LinkedIn profiles that:\n",
    "- Match the known names provided.\n",
    "- Are likely to belong to this company.\n",
    "\n",
    "Search Results:\n",
    "{combined}\n",
    "\"\"\"\n",
    "    \n",
    "    response = client.beta.chat.completions.parse(\n",
    "        model=\"o3-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        response_format=ProfileList\n",
    "    )\n",
    "    return response.choices[0].message.parsed.linkedin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qs7SG2a1CfxS",
    "outputId": "976a0bc8-cc58-4029-a5a7-672764e1023e"
   },
   "outputs": [],
   "source": [
    "def search_and_extract_profiles(queries: List[str], extractor_fn, n=10) -> List[Profile]:\n",
    "    \"\"\"Search and extract profiles using provided extractor function\"\"\"\n",
    "    all_results = []\n",
    "    for query in queries:\n",
    "        all_results.extend(search_google(query, n))\n",
    "    print(f\"All results: {len(all_results)} profiles found\")\n",
    "    return extractor_fn(all_results)\n",
    "\n",
    "def find_profiles(company: str, description: str, titles: List[str], known_names: List[str]) -> List[Profile]:\n",
    "    \"\"\"Main function to find LinkedIn profiles for a company\"\"\"\n",
    "    \n",
    "    if len(known_names) >= 4:\n",
    "        print(\"Using known names approach (4+ founders)\")\n",
    "        queries = [f\"{name} {company} site:linkedin.com/in\" for name in known_names]\n",
    "        print(f'Queries: {queries}')\n",
    "        return search_and_extract_profiles(\n",
    "            queries,\n",
    "            lambda results: extract_profiles_from_search_with_names(results, company, description, titles, known_names),\n",
    "            5\n",
    "        )\n",
    "    else:\n",
    "        # Step 1: Try to use known names if any\n",
    "        profiles = []\n",
    "        if known_names:\n",
    "            print(f\"Using known names: {known_names}\")\n",
    "            queries = [f\"{name} {company} site:linkedin.com/in\" for name in known_names]\n",
    "            profiles.extend(\n",
    "                search_and_extract_profiles(\n",
    "                    queries,\n",
    "                    lambda results: extract_profiles_from_search_with_names(results, company, description, titles, known_names),\n",
    "                    5\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        # Step 2: Use generated queries to discover additional profiles\n",
    "        print(\"Generating additional search queries...\")\n",
    "        generated_queries = generate_search_queries(company, description, titles)\n",
    "        additional_profiles = search_and_extract_profiles(\n",
    "            generated_queries,\n",
    "            lambda results: extract_profiles_from_search(results, company, description, titles)\n",
    "        )\n",
    "        \n",
    "        return profiles + additional_profiles\n",
    "\n",
    "def process_crunchbase_csv(input_csv: str, output_csv: str):\n",
    "    \"\"\"Process Crunchbase data and find LinkedIn profiles\"\"\"\n",
    "    df = pd.read_csv(input_csv)\n",
    "    titles_to_search = [\"CEO\", \"CTO\", \"Co-founder\", \"Vice President\", \"Founder\"]\n",
    "    output_rows = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        company = row['name']\n",
    "        desc = row['full_description']\n",
    "        existing_names = [n.strip() for n in str(row.get('founders', '')).split('|') if n.strip()]\n",
    "        \n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Processing company: {company}\")\n",
    "        print(f\"Existing founders: {existing_names}\")\n",
    "        \n",
    "        try:\n",
    "            profiles = find_profiles(company, desc, titles_to_search, existing_names)\n",
    "            print(f\"Found {len(profiles)} profiles\")\n",
    "            \n",
    "            for profile in profiles:\n",
    "                output_rows.append({\n",
    "                    \"person_name\": profile.name,\n",
    "                    \"linkedin_url\": profile.url,\n",
    "                    **row.to_dict()\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {company}: {e}\")\n",
    "    \n",
    "    # Save results\n",
    "    df_new = pd.DataFrame(output_rows)\n",
    "    df_filtered = df_new.drop_duplicates(subset=[\"name\", \"person_name\"])\n",
    "    df_filtered.to_csv(output_csv, index=False)\n",
    "    print(f\"\\n✅ Profile search complete! Results saved to {output_csv}\")\n",
    "    return df_filtered\n",
    "\n",
    "# Run profile search\n",
    "profiles_df = process_crunchbase_csv(\"company_data.csv\", \"profiles_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 634
    },
    "id": "Nm1cvT9a6shL",
    "outputId": "1dd58058-9114-491a-e828-d7ddbc8cefa7"
   },
   "outputs": [],
   "source": [
    "class LinkedInScraper:\n",
    "    \"\"\"A class to scrape LinkedIn profiles using the Apify LinkedIn Profile Actor.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        apify_token: str,\n",
    "        actor_id: str = \"93aLP1rXaqE97cdr8\",\n",
    "        output_file: str = \"linkedin_profiles.csv\",\n",
    "        wait_for_finish: bool = True,\n",
    "        wait_interval: int = 15,\n",
    "        timeout_seconds: int = 6000,\n",
    "    ):\n",
    "        self.client = ApifyClient(apify_token)\n",
    "        self.actor_id = actor_id\n",
    "        self.output_file = output_file\n",
    "        self.wait_for_finish = wait_for_finish\n",
    "        self.wait_interval = wait_interval\n",
    "        self.timeout_seconds = timeout_seconds\n",
    "    \n",
    "    def validate_linkedin_url(self, url: str) -> bool:\n",
    "        \"\"\"Validate if the URL is a proper LinkedIn profile URL.\"\"\"\n",
    "        try:\n",
    "            if not url:\n",
    "                return False\n",
    "            \n",
    "            # Normalize URL - remove protocol and www for easier checking\n",
    "            url_lower = url.lower()\n",
    "            \n",
    "            # Check if it contains linkedin.com and /in/\n",
    "            if \"linkedin.com\" not in url_lower or \"/in/\" not in url_lower:\n",
    "                return False\n",
    "            \n",
    "            # Parse the URL\n",
    "            parsed_url = urlparse(url)\n",
    "            \n",
    "            # LinkedIn domains: linkedin.com, www.linkedin.com, *.linkedin.com (country codes)\n",
    "            domain = parsed_url.netloc.lower()\n",
    "            valid_domains = (\n",
    "                domain == \"linkedin.com\" or \n",
    "                domain == \"www.linkedin.com\" or\n",
    "                domain.endswith(\".linkedin.com\")\n",
    "            )\n",
    "            \n",
    "            if not valid_domains:\n",
    "                return False\n",
    "                \n",
    "            # Check path structure\n",
    "            path_parts = parsed_url.path.strip(\"/\").split(\"/\")\n",
    "            if len(path_parts) < 2 or path_parts[0] != \"in\":\n",
    "                return False\n",
    "                \n",
    "            # Profile ID should exist and be reasonable length\n",
    "            profile_id = path_parts[1]\n",
    "            if not profile_id or len(profile_id) < 2:\n",
    "                return False\n",
    "                \n",
    "            return True\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "    def prepare_input_data(self, urls: List[str]) -> List[str]:\n",
    "        \"\"\"Prepare the input data list for the Apify actor.\"\"\"\n",
    "        valid_urls = [url for url in urls if self.validate_linkedin_url(url)]\n",
    "        if len(valid_urls) < len(urls):\n",
    "            print(f\"Warning: {len(urls) - len(valid_urls)} invalid URLs were removed.\")\n",
    "        return valid_urls\n",
    "    \n",
    "    def run_actor(self, profile_urls: List[str]) -> Dict[str, Any]:\n",
    "        \"\"\"Run the Apify actor with the given LinkedIn profile URLs.\"\"\"\n",
    "        if not profile_urls:\n",
    "            raise ValueError(\"No valid LinkedIn URLs provided to run the actor.\")\n",
    "        \n",
    "        run_input = {\"profileUrls\": profile_urls}\n",
    "        print(f\"Starting Apify actor ({self.actor_id}) for {len(profile_urls)} profiles...\")\n",
    "        \n",
    "        run = self.client.actor(self.actor_id).call(run_input=run_input)\n",
    "        return run\n",
    "    \n",
    "    def wait_for_run_to_finish(self, run_id: str) -> Optional[Dict[str, Any]]:\n",
    "        \"\"\"Poll the run status until it finishes or times out.\"\"\"\n",
    "        start = time.time()\n",
    "        while True:\n",
    "            run_info = self.client.run(run_id).get()\n",
    "            status = run_info.get(\"status\")\n",
    "            if status in (\"SUCCEEDED\", \"FAILED\", \"ABORTED\"):\n",
    "                return run_info\n",
    "            elapsed = time.time() - start\n",
    "            if elapsed > self.timeout_seconds:\n",
    "                print(f\"Timed out waiting for run {run_id} to finish (>{self.timeout_seconds}s).\")\n",
    "                return None\n",
    "            print(f\"Run {run_id} status: {status}. Checking again in {self.wait_interval}s...\")\n",
    "            time.sleep(self.wait_interval)\n",
    "    \n",
    "    def fetch_results(self, dataset_id: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Fetch all items from the dataset produced by the actor run.\"\"\"\n",
    "        print(f\"Fetching results from dataset {dataset_id}...\")\n",
    "        items = []\n",
    "        for item in self.client.dataset(dataset_id).iterate_items():\n",
    "            items.append(item)\n",
    "        print(f\"Fetched {len(items)} items.\")\n",
    "        return items\n",
    "    \n",
    "    def extract_earliest_year_from_caption(self, caption: str):\n",
    "        \"\"\"Extract earliest year from a caption string\"\"\"\n",
    "        if not caption or not isinstance(caption, str):\n",
    "            return None\n",
    "        \n",
    "        year_matches = [int(part) for part in caption.split() if part.isdigit() and 1900 <= int(part) <= datetime.now().year]\n",
    "        if len(year_matches) >= 1:\n",
    "            return min(year_matches)\n",
    "        return None\n",
    "    \n",
    "    def extract_linkedin_profile_data(self, profiles: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Extracts structured data from a LinkedIn profile dictionary.\"\"\"\n",
    "        flat_data = []\n",
    "        \n",
    "        for profile in profiles:\n",
    "            if not profile:\n",
    "                continue\n",
    "                \n",
    "            base_entry = {\n",
    "                'full_name': profile.get('fullName'),\n",
    "                'headline': profile.get('headline'),\n",
    "                'job_title': profile.get('jobTitle'),\n",
    "                'company': profile.get('companyName'),\n",
    "                'location': profile.get('addressWithCountry'),\n",
    "                'linkedin_url': profile.get('linkedinUrl'),\n",
    "                'about': profile.get('about'),\n",
    "            }\n",
    "            \n",
    "            # Calculate estimated age based on experience and education\n",
    "            experience_years = []\n",
    "            for exp in profile.get('experiences', []):\n",
    "                year = self.extract_earliest_year_from_caption(exp.get('caption'))\n",
    "                if year:\n",
    "                    experience_years.append(year)\n",
    "            \n",
    "            education_years = []\n",
    "            for edu in profile.get('educations', []):\n",
    "                year = self.extract_earliest_year_from_caption(edu.get('caption'))\n",
    "                if year:\n",
    "                    education_years.append(year)\n",
    "            \n",
    "            age = 30  # Default age\n",
    "            if education_years or experience_years:\n",
    "                earliest_edu = min(education_years) if education_years else float('inf')\n",
    "                earliest_exp = min(experience_years) if experience_years else float('inf')\n",
    "                \n",
    "                if earliest_edu < earliest_exp:\n",
    "                    age = datetime.now().year - earliest_edu + 18\n",
    "                else:\n",
    "                    age = datetime.now().year - earliest_exp + 22\n",
    "            \n",
    "            base_entry['estimated_age'] = age\n",
    "            \n",
    "            # Add up to 5 experiences\n",
    "            for i in range(5):\n",
    "                try:\n",
    "                    base_entry[f'experience_{i+1}_title'] = profile['experiences'][i].get('title', '')\n",
    "                    base_entry[f'experience_{i+1}_company'] = profile['experiences'][i].get('subtitle', '')\n",
    "                except (IndexError, KeyError):\n",
    "                    base_entry[f'experience_{i+1}_title'] = ''\n",
    "                    base_entry[f'experience_{i+1}_company'] = ''\n",
    "            \n",
    "            # Add up to 3 educations\n",
    "            for i in range(3):\n",
    "                try:\n",
    "                    base_entry[f'education_{i+1}_school'] = profile['educations'][i].get('title', '')\n",
    "                    base_entry[f'education_{i+1}_degree'] = profile['educations'][i].get('subtitle', '')\n",
    "                except (IndexError, KeyError):\n",
    "                    base_entry[f'education_{i+1}_school'] = ''\n",
    "                    base_entry[f'education_{i+1}_degree'] = ''\n",
    "            \n",
    "            # Add top 5 skills\n",
    "            for i in range(5):\n",
    "                try:\n",
    "                    base_entry[f'skill_{i+1}'] = profile['skills'][i].get('title', '')\n",
    "                except (IndexError, KeyError):\n",
    "                    base_entry[f'skill_{i+1}'] = ''\n",
    "            \n",
    "            flat_data.append(base_entry)\n",
    "        \n",
    "        return flat_data\n",
    "    \n",
    "    def scrape_profiles(self, urls: List[str]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Scrape multiple LinkedIn profiles one by one\"\"\"\n",
    "        valid_urls = self.prepare_input_data(urls)\n",
    "        all_flattened_profiles = []\n",
    "        \n",
    "        for idx, url in enumerate(valid_urls, 1):\n",
    "            print(f\"Processing {idx}/{len(valid_urls)}: {url}\")\n",
    "            try:\n",
    "                run = self.run_actor([url])  # Single URL wrapped in list\n",
    "                run_id = run.get(\"id\")\n",
    "                dataset_id = run.get(\"defaultDatasetId\")\n",
    "                \n",
    "                if self.wait_for_finish:\n",
    "                    final_run_info = self.wait_for_run_to_finish(run_id)\n",
    "                    if not final_run_info or final_run_info.get(\"status\") != \"SUCCEEDED\":\n",
    "                        print(f\"Run {run_id} did not succeed or was aborted.\")\n",
    "                        continue\n",
    "                \n",
    "                if not dataset_id:\n",
    "                    print(f\"No dataset ID for run {run_id}.\")\n",
    "                    continue\n",
    "                \n",
    "                raw_results = self.fetch_results(dataset_id)\n",
    "                flattened = self.extract_linkedin_profile_data(raw_results)\n",
    "                all_flattened_profiles.extend(flattened)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error processing {url}: {e}\")\n",
    "        \n",
    "        return all_flattened_profiles\n",
    "    \n",
    "    def enrich_and_update_existing_csv(self, input_csv_path: str):\n",
    "        \"\"\"Load a CSV with 'person_name' and 'linkedin_url' columns, scrape the profiles, and update the CSV.\"\"\"\n",
    "        df = pd.read_csv(input_csv_path)\n",
    "        \n",
    "        # Filter rows with valid LinkedIn URLs\n",
    "        valid_mask = df[\"linkedin_url\"].apply(self.validate_linkedin_url)\n",
    "        if not valid_mask.any():\n",
    "            print(\"No valid LinkedIn URLs found in the dataset.\")\n",
    "            return\n",
    "        \n",
    "        valid_df = df[valid_mask].copy()\n",
    "        urls = valid_df[\"linkedin_url\"].tolist()\n",
    "        \n",
    "        scraped_profiles = self.scrape_profiles(urls)\n",
    "        if not scraped_profiles:\n",
    "            print(\"No profile data returned.\")\n",
    "            return\n",
    "        \n",
    "        scraped_df = pd.DataFrame(scraped_profiles)\n",
    "        # Merge on linkedin_url\n",
    "        merged_df = pd.merge(df, scraped_df, how=\"left\", left_on=\"linkedin_url\", right_on=\"linkedin_url\")\n",
    "        \n",
    "        merged_df.to_csv(input_csv_path, index=False)\n",
    "        print(f\"Updated dataset saved back to {input_csv_path}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NmiJTwmUZ8HL",
    "outputId": "17f100ae-d16d-4974-f85f-ecc2c7bfed2b"
   },
   "outputs": [],
   "source": [
    "def run_linkedin_scraping():\n",
    "    \"\"\"Run the LinkedIn scraping process\"\"\"\n",
    "    \n",
    "    # Check if profiles_data.csv exists\n",
    "    file_path = \"profiles_data.csv\"\n",
    "    if not os.path.isfile(file_path):\n",
    "        print(f\"Error: {file_path} not found. Please run the profile search first.\")\n",
    "        return None\n",
    "    \n",
    "    # Initialize scraper\n",
    "    scraper = LinkedInScraper(\n",
    "        apify_token=APIFY_TOKEN,\n",
    "        actor_id=\"2SyF0bVxmgGr8IVCZ\"  # Updated actor ID\n",
    "    )\n",
    "    \n",
    "    # Run scraping\n",
    "    print(\"Starting LinkedIn profile scraping...\")\n",
    "    scraper.enrich_and_update_existing_csv(file_path)\n",
    "    \n",
    "    # Load and display results\n",
    "    enriched_df = pd.read_csv(file_path)\n",
    "    print(f\"✅ Scraping complete! {len(enriched_df)} total records processed\")\n",
    "    return enriched_df\n",
    "\n",
    "# Execute scraping (uncomment to run)\n",
    "enriched_df = run_linkedin_scraping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nWHW9lV2Z8D3"
   },
   "outputs": [],
   "source": [
    "class MetricsExtractor:\n",
    "    def __init__(self, api_key, serper_key):\n",
    "        self.api_key = api_key\n",
    "        self.serper_key = serper_key\n",
    "        self.session = None\n",
    "        self.perplexity_url = \"https://api.perplexity.ai/chat/completions\"\n",
    "        self.model = \"sonar-reasoning\"\n",
    "    \n",
    "    async def initialize(self):\n",
    "        if not self.session:\n",
    "            self.session = aiohttp.ClientSession()\n",
    "    \n",
    "    async def clean_cache(self):\n",
    "        if self.session:\n",
    "            await self.session.close()\n",
    "        self.session = None\n",
    "    \n",
    "    async def search_info(self, query, year, region=\"us\"):\n",
    "        if not self.session:\n",
    "            await self.initialize()\n",
    "        \n",
    "        # Add timestamp to avoid caching\n",
    "        timestamp = int(time.time())\n",
    "        query = f\"{query} {timestamp}\"\n",
    "        \n",
    "        payload = {\n",
    "            \"q\": query,\n",
    "            \"gl\": region,\n",
    "            \"num\": 10,\n",
    "            \"page\": 0\n",
    "        }\n",
    "        \n",
    "        headers = {\n",
    "            'X-API-KEY': self.serper_key,\n",
    "            'Content-Type': 'application/json',\n",
    "            'Cache-Control': 'no-cache'\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            async with self.session.post(\n",
    "                \"https://google.serper.dev/search\",\n",
    "                headers=headers,\n",
    "                json=payload\n",
    "            ) as response:\n",
    "                if response.status == 200:\n",
    "                    return await response.json()\n",
    "                else:\n",
    "                    print(f\"Search API error: {response.status}\")\n",
    "                    return {\"organic\": []}\n",
    "        except Exception as e:\n",
    "            print(f\"Search error: {str(e)}\")\n",
    "            return {\"organic\": []}\n",
    "            \n",
    "    async def get_market_metrics(self, sector_name, year):\n",
    "        \"\"\"Extract market size and CAGR metrics\"\"\"\n",
    "        queries = [\n",
    "            f\"{sector_name} market size {year}\",\n",
    "            f\"{sector_name} market value {year}\",\n",
    "            f\"{sector_name} industry growth rate\",\n",
    "            f\"{sector_name} market CAGR forecast\",\n",
    "            f\"{sector_name} market analysis report\"\n",
    "        ]\n",
    "        \n",
    "        print(\"\\nAnalyzing market metrics...\")\n",
    "        all_text = []\n",
    "        seen_urls = set()\n",
    "        \n",
    "        for query in queries:\n",
    "            try:\n",
    "                results = await self.search_info(query, year)\n",
    "                if isinstance(results, dict) and \"organic\" in results:\n",
    "                    for item in results[\"organic\"]:\n",
    "                        url = item.get(\"link\", \"\").strip()\n",
    "                        if not url or url in seen_urls:\n",
    "                            continue\n",
    "                        seen_urls.add(url)\n",
    "                        \n",
    "                        text = f\"{item.get('title', '')} {item.get('snippet', '')}\"\n",
    "                        if sector_name.lower() in text.lower():\n",
    "                            all_text.append(text)\n",
    "                            print(f\"\\nRelevant market data found:\")\n",
    "                            print(f\"Title: {item.get('title', '')}\")\n",
    "                            print(f\"Snippet: {item.get('snippet', '')}\")\n",
    "                            print(f\"URL: {item.get('link', '')}\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Search error: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        combined_text = \" \".join(all_text)\n",
    "        \n",
    "        # Market size patterns\n",
    "        market_size_patterns = [\n",
    "            r'market\\s+(?:size|value)\\s+(?:of\\s+)?(\\$|\\€|\\£|USD|EUR|GBP|US$)\\s*([\\d,\\.]+)\\s*(billion|bn|b)',\n",
    "            r'market\\s+(?:size|value)\\s+(?:of\\s+)?(\\$|\\€|\\£|USD|EUR|GBP|US$)\\s*([\\d,\\.]+)\\s*(million|mn|m)',\n",
    "            r'(?:valued|estimated)\\s+(?:at\\s+)?(\\$|\\€|\\£|USD|EUR|GBP|US$)\\s*([\\d,\\.]+)\\s*(billion|bn|b)',\n",
    "            r'(?:valued|estimated)\\s+(?:at\\s+)?(\\$|\\€|\\£|USD|EUR|GBP|US$)\\s*([\\d,\\.]+)\\s*(million|mn|m)',\n",
    "            r'worth\\s+(\\$|\\€|\\£|USD|EUR|GBP|US$)\\s*([\\d,\\.]+)\\s*(billion|bn|b)',\n",
    "            r'worth\\s+(\\$|\\€|\\£|USD|EUR|GBP|US$)\\s*([\\d,\\.]+)\\s*(million|mn|m)'\n",
    "        ]\n",
    "        \n",
    "        # CAGR patterns\n",
    "        cagr_patterns = [\n",
    "            r'CAGR\\s+of\\s+([\\d\\.]+)\\s*%',\n",
    "            r'compound\\s+annual\\s+growth\\s+rate\\s+(?:\\(CAGR\\))?\\s+of\\s+([\\d\\.]+)\\s*%',\n",
    "            r'growth\\s+rate\\s+of\\s+([\\d\\.]+)\\s*%\\s+(?:annually|per\\s+year)'\n",
    "        ]\n",
    "        \n",
    "        # Extract market sizes\n",
    "        currency_rates = {\n",
    "            '$': 1.0, 'USD': 1.0, 'US$': 1.0,\n",
    "            '€': 1.1, 'EUR': 1.1,\n",
    "            '£': 1.25, 'GBP': 1.25\n",
    "        }\n",
    "        market_sizes = []\n",
    "        for pattern in market_size_patterns:\n",
    "            matches = re.finditer(pattern, combined_text, re.IGNORECASE)\n",
    "            for match in matches:\n",
    "                try:\n",
    "                    currency = match.group(1).upper()\n",
    "                    value = float(match.group(2).replace(',', ''))\n",
    "                    unit = match.group(3).lower()\n",
    "                    if unit in ['million', 'mn', 'm']:\n",
    "                        value /= 1000\n",
    "                    fx = currency_rates.get(currency, 1)\n",
    "                    value *= fx\n",
    "                    \n",
    "                    if 0 < value < 10000:\n",
    "                        market_sizes.append(value)\n",
    "                except:\n",
    "                    continue\n",
    "    \n",
    "        # Extract CAGR values\n",
    "        cagr_values = []\n",
    "        for pattern in cagr_patterns:\n",
    "            matches = re.finditer(pattern, combined_text, re.IGNORECASE)\n",
    "            for match in matches:\n",
    "                try:\n",
    "                    value = float(match.group(1))\n",
    "                    if 0 < value < 100:\n",
    "                        cagr_values.append(value)\n",
    "                except:\n",
    "                    continue\n",
    "        \n",
    "        # Calculate metrics\n",
    "        market_size = 0\n",
    "        cagr = 0\n",
    "        \n",
    "        if market_sizes:\n",
    "            market_size = statistics.median(market_sizes)\n",
    "        \n",
    "        if cagr_values:\n",
    "            cagr = statistics.median(cagr_values)\n",
    "        \n",
    "        print(\"\\nMarket Metrics Analysis:\")\n",
    "        print(f\"Market sizes found: {market_sizes}\")\n",
    "        print(f\"CAGR values found: {cagr_values}\")\n",
    "        print(f\"\\nFinal Metrics:\")\n",
    "        print(f\"Market Size: ${market_size:.2f} billion\")\n",
    "        print(f\"CAGR: {cagr:.1f}%\")\n",
    "        \n",
    "        return {\n",
    "            \"market_size\": market_size,\n",
    "            \"cagr\": cagr\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "nkBvV3YPQSUx",
    "outputId": "e9b9144f-724a-4ca6-a0db-900f14cd74d1"
   },
   "outputs": [],
   "source": [
    "async def get_timing_score(self, sector_name, year, cagr):\n",
    "    \"\"\"Calculate market timing score based on multiple indicators\"\"\"\n",
    "    # Search queries focused on timing indicators\n",
    "    queries = [\n",
    "        f\"{sector_name} market readiness {year}\",\n",
    "        f\"{sector_name} technology adoption stage {year}\",\n",
    "        f\"{sector_name} market maturity analysis\",\n",
    "        f\"{sector_name} industry growth phase\",\n",
    "        f\"{sector_name} market barriers entry\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nAnalyzing market timing factors...\")\n",
    "    all_text = []\n",
    "    seen_urls = set()\n",
    "    \n",
    "    # Collect timing data\n",
    "    for query in queries:\n",
    "        try:\n",
    "            results = await self.search_info(query, year)\n",
    "            if isinstance(results, dict) and \"organic\" in results:\n",
    "                for item in results[\"organic\"]:\n",
    "                    url = item.get(\"link\", \"\").strip()\n",
    "                    if not url or url in seen_urls:\n",
    "                        continue\n",
    "                    seen_urls.add(url)\n",
    "                    \n",
    "                    text = f\"{item.get('title', '')} {item.get('snippet', '')}\"\n",
    "                    if sector_name.lower() in text.lower():\n",
    "                        all_text.append(text)\n",
    "                        print(f\"\\nRelevant timing indicator found:\")\n",
    "                        print(f\"Title: {item.get('title', '')}\")\n",
    "                        print(f\"Snippet: {item.get('snippet', '')}\")\n",
    "                        print(f\"URL: {item.get('link', '')}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Search error: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    combined_text = \" \".join(all_text).lower()\n",
    "    \n",
    "    # Market phase indicators\n",
    "    phase_indicators = {\n",
    "        'early': {\n",
    "            'terms': ['emerging', 'nascent', 'early stage', 'beginning', 'experimental',\n",
    "                     'pilot', 'prototype', 'research phase', 'early adoption',\n",
    "                     'innovative', 'breakthrough', 'pioneering', 'cutting edge'],\n",
    "            'weight': 1.0\n",
    "        },\n",
    "        'growth': {\n",
    "            'terms': ['growing', 'expansion', 'scaling', 'increasing adoption',\n",
    "                     'rapid growth', 'accelerating', 'momentum', 'traction',\n",
    "                     'market penetration', 'widespread adoption'],\n",
    "            'weight': 0.8\n",
    "        },\n",
    "        'mature': {\n",
    "            'terms': ['mature', 'established', 'stable', 'saturated', 'consolidated',\n",
    "                     'mainstream', 'traditional', 'conventional', 'standardized'],\n",
    "            'weight': 0.6\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Calculate phase scores\n",
    "    phase_scores = {phase: 0 for phase in phase_indicators}\n",
    "    for phase, data in phase_indicators.items():\n",
    "        score = 0\n",
    "        for term in data['terms']:\n",
    "            if term in combined_text:\n",
    "                score += 1\n",
    "        phase_scores[phase] = score * data['weight']\n",
    "    \n",
    "    # Normalize phase scores\n",
    "    total_score = sum(phase_scores.values()) or 1\n",
    "    phase_scores = {phase: score/total_score for phase, score in phase_scores.items()}\n",
    "    \n",
    "    # Market readiness indicators\n",
    "    readiness_indicators = {\n",
    "        'positive': {\n",
    "            'terms': ['ready', 'prepared', 'opportunity', 'potential', 'promising',\n",
    "                     'favorable', 'advantageous', 'strategic', 'beneficial'],\n",
    "            'weight': 1.0\n",
    "        },\n",
    "        'negative': {\n",
    "            'terms': ['barrier', 'challenge', 'obstacle', 'limitation', 'constraint',\n",
    "                     'risk', 'concern', 'problem', 'issue', 'difficulty'],\n",
    "            'weight': -0.5\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Calculate readiness score\n",
    "    readiness_score = 0\n",
    "    for sentiment, data in readiness_indicators.items():\n",
    "        for term in data['terms']:\n",
    "            if term in combined_text:\n",
    "                readiness_score += data['weight']\n",
    "    \n",
    "    # Normalize readiness score\n",
    "    readiness_score = max(0, min(1, (readiness_score + 3) / 6))\n",
    "    \n",
    "    # Calculate timing score components\n",
    "    emerging_tech = ['quantum', 'ai', 'blockchain', 'robotics']\n",
    "    is_emerging = any(tech in sector_name.lower() for tech in emerging_tech)\n",
    "    \n",
    "    # Adjust CAGR expectations based on sector type\n",
    "    if is_emerging:\n",
    "        cagr_max = 60.0\n",
    "    else:\n",
    "        cagr_max = 40.0\n",
    "    \n",
    "    # Calculate normalized CAGR score\n",
    "    cagr_score = min(cagr / cagr_max, 1.0)\n",
    "    \n",
    "    # Calculate final timing score\n",
    "    phase_component = (phase_scores['early'] * 1.0 +\n",
    "                     phase_scores['growth'] * 0.8 +\n",
    "                     phase_scores['mature'] * 0.6)\n",
    "    \n",
    "    timing_score = (cagr_score * 0.4 +           # CAGR importance\n",
    "                   phase_component * 0.3 +        # Market phase\n",
    "                   readiness_score * 0.3) * 5     # Market readiness\n",
    "    \n",
    "    # Ensure score is within 1-5 range\n",
    "    timing_score = round(max(1.0, min(5.0, timing_score)), 1)\n",
    "    \n",
    "    print(\"\\nTiming Score Analysis:\")\n",
    "    print(f\"CAGR Score: {cagr_score:.2f}\")\n",
    "    print(f\"Market Phase Distribution: {phase_scores}\")\n",
    "    print(f\"Market Readiness Score: {readiness_score:.2f}\")\n",
    "    print(f\"\\nFinal Timing Score: {timing_score}/5\")\n",
    "    \n",
    "    return timing_score\n",
    "\n",
    "# Add this method to MetricsExtractor class\n",
    "MetricsExtractor.get_timing_score = get_timing_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_regional_sentiment(self, sector_name, year, region):\n",
    "    \"\"\"Calculate regional market sentiment score\"\"\"\n",
    "    # Region-specific configurations\n",
    "    region_configs = {\n",
    "        'us': {\n",
    "            'country': 'United States',\n",
    "            'region_terms': ['US', 'USA', 'United States', 'North America', 'American'],\n",
    "            'gl': 'us',\n",
    "            'cities': ['Silicon Valley', 'New York', 'Boston', 'Seattle', 'Austin']\n",
    "        },\n",
    "        'sg': {\n",
    "            'country': 'Southeast Asia',\n",
    "            'region_terms': ['Southeast Asia', 'ASEAN', 'Singapore', 'Indonesia', 'Malaysia', 'Thailand'],\n",
    "            'gl': 'sg',\n",
    "            'cities': ['Singapore', 'Jakarta', 'Bangkok', 'Kuala Lumpur', 'Ho Chi Minh']\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Get region configuration\n",
    "    region_config = region_configs.get(region.lower())\n",
    "    if not region_config:\n",
    "        print(f\"Warning: Unsupported region '{region}'. Using default values.\")\n",
    "        return 3.0\n",
    "    \n",
    "    # Enhanced search queries\n",
    "    queries = [\n",
    "        f\"{sector_name} market growth {region_config['country']} {year}\",\n",
    "        f\"{sector_name} investment trends {region_config['country']}\",\n",
    "        f\"{sector_name} adoption rate {region_config['country']}\",\n",
    "        f\"{sector_name} market opportunity {region_config['country']}\",\n",
    "        f\"{sector_name} industry development {region_config['country']}\"\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\nAnalyzing {region_config['country']} market sentiment...\")\n",
    "    all_text = []\n",
    "    seen_urls = set()\n",
    "    \n",
    "    # Collect regional data\n",
    "    for query in queries:\n",
    "        try:\n",
    "            results = await self.search_info(query, year, region=region_config['gl'])\n",
    "            if isinstance(results, dict) and \"organic\" in results:\n",
    "                for item in results[\"organic\"]:\n",
    "                    url = item.get(\"link\", \"\").strip()\n",
    "                    if not url or url in seen_urls:\n",
    "                        continue\n",
    "                    seen_urls.add(url)\n",
    "                    \n",
    "                    text = f\"{item.get('title', '')} {item.get('snippet', '')}\"\n",
    "                    if (sector_name.lower() in text.lower() and\n",
    "                        any(term.lower() in text.lower() for term in region_config['region_terms'])):\n",
    "                        all_text.append(text)\n",
    "                        print(f\"\\nRelevant regional indicator found:\")\n",
    "                        print(f\"Title: {item.get('title', '')}\")\n",
    "                        print(f\"Snippet: {item.get('snippet', '')}\")\n",
    "                        print(f\"URL: {item.get('link', '')}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Search error: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    combined_text = \" \".join(all_text).lower()\n",
    "    \n",
    "    # Sentiment indicators\n",
    "    sentiment_indicators = {\n",
    "        'positive': {\n",
    "            'terms': [\n",
    "                'growth', 'opportunity', 'expansion', 'investment',\n",
    "                'innovation', 'development', 'success', 'potential',\n",
    "                'promising', 'favorable', 'strategic', 'advantage'\n",
    "            ],\n",
    "            'weight': 1.0\n",
    "        },\n",
    "        'negative': {\n",
    "            'terms': [\n",
    "                'challenge', 'barrier', 'limitation', 'restriction',\n",
    "                'regulation', 'constraint', 'risk', 'concern',\n",
    "                'problem', 'difficulty', 'obstacle', 'threat'\n",
    "            ],\n",
    "            'weight': -0.7\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Calculate sentiment score\n",
    "    sentiment_score = 0\n",
    "    for sentiment, data in sentiment_indicators.items():\n",
    "        for term in data['terms']:\n",
    "            if term in combined_text:\n",
    "                sentiment_score += data['weight']\n",
    "    \n",
    "    # Normalize sentiment score\n",
    "    sentiment_score = max(0, min(1, (sentiment_score + 5) / 10))\n",
    "    \n",
    "    # Calculate regional presence\n",
    "    presence_score = 0\n",
    "    for city in region_config['cities']:\n",
    "        if city.lower() in combined_text:\n",
    "            presence_score += 0.2\n",
    "    presence_score = min(1.0, presence_score)\n",
    "    \n",
    "    # Calculate final score\n",
    "    final_score = (sentiment_score * 0.7 + presence_score * 0.3) * 5\n",
    "    final_score = round(max(1.0, min(5.0, final_score)), 1)\n",
    "    \n",
    "    print(f\"\\nRegional Sentiment Analysis for {region_config['country']}:\")\n",
    "    print(f\"Sentiment Score: {sentiment_score:.2f}\")\n",
    "    print(f\"Regional Presence Score: {presence_score:.2f}\")\n",
    "    print(f\"\\nFinal Regional Sentiment Score: {final_score}/5\")\n",
    "    \n",
    "    return final_score\n",
    "\n",
    "# Add this method to MetricsExtractor class\n",
    "MetricsExtractor.get_regional_sentiment = get_regional_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_competitor_metrics(self, sector_name, year):\n",
    "    \"\"\"Extract competitor metrics with enhanced validation\"\"\"\n",
    "    # Comprehensive search queries\n",
    "    queries = [\n",
    "        f\"{sector_name} market leaders revenue {year}\",\n",
    "        f\"{sector_name} industry players market share {year}\",\n",
    "        f\"{sector_name} companies competitive landscape {year}\",\n",
    "        f\"{sector_name} market structure analysis {year}\",\n",
    "        f\"{sector_name} venture funding deals {year}\",\n",
    "        f\"{sector_name} startup investment rounds {year}\",\n",
    "        f\"{sector_name} industry consolidation {year}\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nAnalyzing competitor metrics...\")\n",
    "    all_text = []\n",
    "    seen_urls = set()\n",
    "    \n",
    "    # Collect competitor data\n",
    "    for query in queries:\n",
    "        try:\n",
    "            results = await self.search_info(query, year)\n",
    "            if isinstance(results, dict) and \"organic\" in results:\n",
    "                for item in results[\"organic\"]:\n",
    "                    url = item.get(\"link\", \"\").strip()\n",
    "                    if not url or url in seen_urls:\n",
    "                        continue\n",
    "                    seen_urls.add(url)\n",
    "                    text = f\"{item.get('title', '')} {item.get('snippet', '')}\"\n",
    "                    if sector_name.lower() in text.lower():\n",
    "                        all_text.append(text)\n",
    "                        print(f\"\\nRelevant competitor data found:\")\n",
    "                        print(f\"Title: {item.get('title', '')}\")\n",
    "                        print(f\"Snippet: {item.get('snippet', '')}\")\n",
    "                        print(f\"URL: {item.get('link', '')}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Search error: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    combined_text = \" \".join(all_text)\n",
    "    \n",
    "    # Enhanced patterns\n",
    "    patterns = {\n",
    "        'competitors': [\n",
    "            r'([\\d,]+)\\s+(?:major|key|leading|primary|main)\\s+(?:players|companies|competitors)',\n",
    "            r'(?:market|industry)\\s+has\\s+([\\d,]+)\\s+(?:players|companies|competitors)',\n",
    "            r'top\\s+([\\d,]+)\\s+(?:players|companies|competitors)',\n",
    "            r'approximately\\s+([\\d,]+)\\s+(?:players|companies|competitors)',\n",
    "            r'more\\s+than\\s+([\\d,]+)\\s+(?:players|companies)',\n",
    "            r'among\\s+([\\d,]+)\\s+(?:players|companies|competitors)',\n",
    "            r'comprises\\s+([\\d,]+)\\s+(?:players|companies|competitors)'\n",
    "        ],\n",
    "        'funding': [\n",
    "            r'[\\$\\€\\£]\\s*([\\d,\\.]+)\\s*(billion|bn|b)\\s+(?:funding|investment)',\n",
    "            r'[\\$\\€\\£]\\s*([\\d,\\.]+)\\s*(million|mn|m)\\s+(?:funding|investment)',\n",
    "            r'(?:funding|investment)\\s+of\\s+[\\$\\€\\£]\\s*([\\d,\\.]+)\\s*(billion|bn|b)',\n",
    "            r'(?:funding|investment)\\s+of\\s+[\\$\\€\\£]\\s*([\\d,\\.]+)\\s*(million|mn|m)',\n",
    "            r'raised\\s+[\\$\\€\\£]\\s*([\\d,\\.]+)\\s*(billion|bn|b)',\n",
    "            r'raised\\s+[\\$\\€\\£]\\s*([\\d,\\.]+)\\s*(million|mn|m)'\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Extract competitor counts\n",
    "    competitor_counts = []\n",
    "    for pattern in patterns['competitors']:\n",
    "        matches = re.finditer(pattern, combined_text, re.IGNORECASE)\n",
    "        for match in matches:\n",
    "            try:\n",
    "                count = float(match.group(1).replace(',', ''))\n",
    "                if 2 <= count <= 1000:  # Reasonable range\n",
    "                    competitor_counts.append(count)\n",
    "            except:\n",
    "                continue\n",
    "    \n",
    "    # Extract funding values\n",
    "    funding_values = []\n",
    "    for pattern in patterns['funding']:\n",
    "        matches = re.finditer(pattern, combined_text, re.IGNORECASE)\n",
    "        for match in matches:\n",
    "            try:\n",
    "                value = float(match.group(1).replace(',', ''))\n",
    "                unit = match.group(2).lower()\n",
    "                if unit in ['million', 'mn', 'm']:\n",
    "                    value /= 1000\n",
    "                if 0 < value < 1000:  # Reasonable range in billions\n",
    "                    funding_values.append(value)\n",
    "            except:\n",
    "                continue\n",
    "    \n",
    "    # Calculate metrics with validation\n",
    "    def calculate_metric(values, default, min_valid=2):\n",
    "        \"\"\"Calculate metric with outlier removal\"\"\"\n",
    "        if len(values) < min_valid:\n",
    "            return default, 0.0\n",
    "        \n",
    "        if len(values) > 3:\n",
    "            # Remove outliers\n",
    "            q1, q3 = statistics.quantiles(values)[0], statistics.quantiles(values)[2]\n",
    "            iqr = q3 - q1\n",
    "            lower_bound = q1 - 1.5 * iqr\n",
    "            upper_bound = q3 + 1.5 * iqr\n",
    "            values = [x for x in values if lower_bound <= x <= upper_bound]\n",
    "        \n",
    "        confidence = min(len(values) / 5, 1.0)\n",
    "        return statistics.median(values) if values else default, confidence\n",
    "    \n",
    "    # Calculate final metrics\n",
    "    competitor_count, comp_confidence = calculate_metric(competitor_counts, 10)\n",
    "    competitor_count = int(competitor_count)\n",
    "    total_funding, fund_confidence = calculate_metric(funding_values, 1.0)\n",
    "    \n",
    "    # Calculate momentum score\n",
    "    try:\n",
    "        emerging_tech = ['quantum computing', 'artificial intelligence', 'blockchain']\n",
    "        is_emerging = any(tech in sector_name.lower() for tech in emerging_tech)\n",
    "        \n",
    "        if is_emerging:\n",
    "            comp_scale = 35.0\n",
    "            fund_scale = 25.0\n",
    "            fund_weight = 0.7\n",
    "        else:\n",
    "            comp_scale = 25.0\n",
    "            fund_scale = 15.0\n",
    "            fund_weight = 0.6\n",
    "        \n",
    "        comp_weight = 1.0 - fund_weight\n",
    "        \n",
    "        norm_competitors = min(competitor_count / comp_scale, 1.0)\n",
    "        norm_funding = min(total_funding / fund_scale, 1.0)\n",
    "        \n",
    "        total_confidence = (comp_confidence * comp_weight + fund_confidence * fund_weight)\n",
    "        \n",
    "        base_score = (norm_competitors * comp_weight + norm_funding * fund_weight) * 5\n",
    "        momentum_score = base_score * total_confidence + 3.0 * (1 - total_confidence)\n",
    "        momentum_score = round(max(1.0, min(5.0, momentum_score)), 1)\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating momentum score: {str(e)}\")\n",
    "        momentum_score = 3.0\n",
    "        total_confidence = 0.0\n",
    "    \n",
    "    print(\"\\nCompetitor Metrics Analysis:\")\n",
    "    print(f\"Competitor counts found: {competitor_counts}\")\n",
    "    print(f\"Funding values found: {funding_values}\")\n",
    "    print(f\"\\nConfidence metrics:\")\n",
    "    print(f\"Competitor confidence: {comp_confidence:.2f}\")\n",
    "    print(f\"Funding confidence: {fund_confidence:.2f}\")\n",
    "    print(f\"\\nFinal metrics:\")\n",
    "    print(f\"Competitor count: {competitor_count}\")\n",
    "    print(f\"Total funding: ${total_funding:.2f}B\")\n",
    "    print(f\"Momentum score: {momentum_score}/5\")\n",
    "    \n",
    "    return {\n",
    "        \"competitor_count\": competitor_count,\n",
    "        \"total_funding\": total_funding,\n",
    "        \"momentum_score\": momentum_score\n",
    "    }\n",
    "\n",
    "# Add this method to MetricsExtractor class\n",
    "MetricsExtractor.get_competitor_metrics = get_competitor_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def extract_all_metrics(self, sector_name, year):\n",
    "    \"\"\"Extract all metrics with enhanced error handling and parallel execution\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        print(f\"\\nExtracting metrics for {sector_name} sector in {year}...\")\n",
    "        \n",
    "        # Get market metrics first (market size and CAGR)\n",
    "        print(\"\\nFetching market metrics...\")\n",
    "        market_metrics = await self.get_market_metrics(sector_name, year)\n",
    "        \n",
    "        # Create tasks for parallel execution\n",
    "        tasks = [\n",
    "            self.get_timing_score(sector_name, year, market_metrics[\"cagr\"]),\n",
    "            self.get_regional_sentiment(sector_name, year, \"us\"),\n",
    "            self.get_regional_sentiment(sector_name, year, \"sg\"),\n",
    "            self.get_competitor_metrics(sector_name, year)\n",
    "        ]\n",
    "        \n",
    "        # Execute remaining tasks in parallel\n",
    "        timing_score, us_sentiment, sea_sentiment, competitor_metrics = await asyncio.gather(*tasks)\n",
    "        \n",
    "        # Combine all metrics with validation\n",
    "        metrics = {\n",
    "            \"market_size\": float(market_metrics.get(\"market_size\", 0) or 0),\n",
    "            \"cagr\": float(market_metrics.get(\"cagr\", 0) or 0),\n",
    "            \"timing_score\": float(timing_score or 3.0),\n",
    "            \"us_sentiment\": float(us_sentiment or 3.0),\n",
    "            \"sea_sentiment\": float(sea_sentiment or 3.0),\n",
    "            \"competitor_count\": int(competitor_metrics.get(\"competitor_count\", 10) or 10),\n",
    "            \"total_funding\": float(competitor_metrics.get(\"total_funding\", 1.0) or 1.0),\n",
    "            \"momentum_score\": float(competitor_metrics.get(\"momentum_score\", 3.0) or 3.0),\n",
    "            \"execution_time\": time.time() - start_time\n",
    "        }\n",
    "        \n",
    "        # Print summary\n",
    "        print(\"\\n===== METRICS EXTRACTION SUMMARY =====\")\n",
    "        print(f\"Sector: {sector_name}\")\n",
    "        print(f\"Year: {year}\")\n",
    "        print(f\"\\nMarket Metrics:\")\n",
    "        print(f\"- Market Size: ${metrics['market_size']:.2f} billion\")\n",
    "        print(f\"- CAGR: {metrics['cagr']:.1f}%\")\n",
    "        print(f\"\\nTiming and Sentiment:\")\n",
    "        print(f\"- Market Timing Score: {metrics['timing_score']:.1f}/5\")\n",
    "        print(f\"- US Market Sentiment: {metrics['us_sentiment']:.1f}/5\")\n",
    "        print(f\"- SEA Market Sentiment: {metrics['sea_sentiment']:.1f}/5\")\n",
    "        print(f\"\\nCompetitor Metrics:\")\n",
    "        print(f\"- Number of Competitors: {metrics['competitor_count']}\")\n",
    "        print(f\"- Total Funding: ${metrics['total_funding']:.2f} billion\")\n",
    "        print(f\"- Market Momentum Score: {metrics['momentum_score']:.1f}/5\")\n",
    "        print(f\"\\nExecution Time: {metrics['execution_time']:.2f} seconds\")\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError in extract_all_metrics: {str(e)}\")\n",
    "        # Return default values if something goes wrong\n",
    "        return {\n",
    "            \"market_size\": 0,\n",
    "            \"cagr\": 0,\n",
    "            \"timing_score\": 3.0,\n",
    "            \"us_sentiment\": 3.0,\n",
    "            \"sea_sentiment\": 3.0,\n",
    "            \"competitor_count\": 10,\n",
    "            \"total_funding\": 1.0,\n",
    "            \"momentum_score\": 3.0,\n",
    "            \"execution_time\": time.time() - start_time\n",
    "        }\n",
    "\n",
    "# Add this method to MetricsExtractor class\n",
    "MetricsExtractor.extract_all_metrics = extract_all_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_year(date_str):\n",
    "    \"\"\"Extract year from date string\"\"\"\n",
    "    try:\n",
    "        return pd.to_datetime(date_str).year\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def clean_think(text):\n",
    "    \"\"\"Remove <think> tags from LLM output\"\"\"\n",
    "    return re.sub(r\"<think>.*?</think>\", \"\", text, flags=re.DOTALL).strip()\n",
    "\n",
    "async def get_sector_from_llm(description, session, api_key):\n",
    "    \"\"\"Get sector classification from LLM\"\"\"\n",
    "    url = \"https://api.perplexity.ai/chat/completions\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {api_key}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"\"\"\n",
    "Given the following company description, respond only with the industry sector name (no more than 10 words).\n",
    "Do NOT include full sentences, company names, or explanations. Just return the sector name.\n",
    "\n",
    "Description: {description}\n",
    "\"\"\"\n",
    "    }]\n",
    "    payload = {\n",
    "        \"model\": \"sonar-reasoning\",\n",
    "        \"messages\": messages\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        async with session.post(url, headers=headers, json=payload) as resp:\n",
    "            res = await resp.json()\n",
    "            raw_output = res['choices'][0]['message']['content']\n",
    "            return clean_think(raw_output)\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting sector from LLM: {e}\")\n",
    "        return \"Technology\"\n",
    "\n",
    "def validate_data_integrity(df, required_columns):\n",
    "    \"\"\"Validate data integrity before processing\"\"\"\n",
    "    missing_cols = [col for col in required_columns if col not in df.columns]\n",
    "    if missing_cols:\n",
    "        print(f\"Warning: Missing columns: {missing_cols}\")\n",
    "        return False\n",
    "    \n",
    "    print(f\"✅ Data validation passed. {len(df)} records with all required columns.\")\n",
    "    return True\n",
    "\n",
    "def save_checkpoint(data, filename):\n",
    "    \"\"\"Save checkpoint data for recovery\"\"\"\n",
    "    try:\n",
    "        if isinstance(data, pd.DataFrame):\n",
    "            data.to_csv(f\"checkpoint_{filename}.csv\", index=False)\n",
    "        else:\n",
    "            import pickle\n",
    "            with open(f\"checkpoint_{filename}.pkl\", 'wb') as f:\n",
    "                pickle.dump(data, f)\n",
    "        print(f\"✅ Checkpoint saved: checkpoint_{filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error saving checkpoint: {e}\")\n",
    "\n",
    "def load_checkpoint(filename, file_type='csv'):\n",
    "    \"\"\"Load checkpoint data for recovery\"\"\"\n",
    "    try:\n",
    "        if file_type == 'csv':\n",
    "            return pd.read_csv(f\"checkpoint_{filename}.csv\")\n",
    "        else:\n",
    "            import pickle\n",
    "            with open(f\"checkpoint_{filename}.pkl\", 'rb') as f:\n",
    "                return pickle.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"No checkpoint found: checkpoint_{filename}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading checkpoint: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_analysis_and_save(api_key, serper_key, input_file=\"profiles_data.csv\", output_file=\"profiles_data_with_metrics.csv\"):\n",
    "    \"\"\"Run the complete market analysis and save results\"\"\"\n",
    "    \n",
    "    # Check if input file exists\n",
    "    if not os.path.isfile(input_file):\n",
    "        print(f\"Error: {input_file} not found. Please run the previous steps first.\")\n",
    "        return None\n",
    "    \n",
    "    # Load the data\n",
    "    df = pd.read_csv(input_file)\n",
    "    print(f\"Loaded {len(df)} records from {input_file}\")\n",
    "    \n",
    "    # Cache to store processed companies\n",
    "    company_cache = {}\n",
    "    \n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        extractor = MetricsExtractor(api_key, serper_key)\n",
    "        await extractor.initialize()\n",
    "        \n",
    "        # Get unique companies\n",
    "        unique_companies = df['name'].unique() if 'name' in df.columns else df['name_x'].unique()\n",
    "        print(f\"Found {len(unique_companies)} unique companies to analyze\")\n",
    "        \n",
    "        for idx, name in enumerate(unique_companies, 1):\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"PROCESSING COMPANY {idx}/{len(unique_companies)}: {name}\")\n",
    "            print(f\"{'='*60}\")\n",
    "            \n",
    "            # Get company data\n",
    "            company_mask = (df['name'] == name) if 'name' in df.columns else (df['name_x'] == name)\n",
    "            sub_df = df[company_mask]\n",
    "            \n",
    "            if len(sub_df) == 0:\n",
    "                print(f\"No data found for company: {name}\")\n",
    "                continue\n",
    "            \n",
    "            # Extract company information\n",
    "            description = sub_df.iloc[0].get('full_description', '')\n",
    "            founded = sub_df.iloc[0].get(\"founded_on\", None)\n",
    "            year = extract_year(founded) or 2020  # Default to 2020 if no date\n",
    "            \n",
    "            if not description:\n",
    "                print(f\"No description found for {name}, skipping...\")\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                # Get sector classification\n",
    "                print(\"Getting sector classification...\")\n",
    "                sector = await get_sector_from_llm(description, session, api_key)\n",
    "                print(f\"Classified sector: {sector}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error getting sector classification: {e}\")\n",
    "                sector = \"Technology\"\n",
    "            \n",
    "            print(f\"Analyzing: {name}\")\n",
    "            print(f\"Sector: {sector} | Founded: {year}\")\n",
    "            print(f\"Description: {description[:200]}...\")\n",
    "            \n",
    "            try:\n",
    "                # Extract all metrics\n",
    "                metrics = await extractor.extract_all_metrics(sector, year)\n",
    "                print(f\"✅ Successfully extracted metrics for {name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error extracting metrics for {name}: {e}\")\n",
    "                # Use default values\n",
    "                metrics = {\n",
    "                    \"market_size\": 0, \"cagr\": 0, \"timing_score\": 3,\n",
    "                    \"us_sentiment\": 3, \"sea_sentiment\": 3,\n",
    "                    \"competitor_count\": 10, \"total_funding\": 1.0,\n",
    "                    \"momentum_score\": 3.0, \"execution_time\": 0\n",
    "                }\n",
    "            \n",
    "            # Store in cache\n",
    "            company_cache[name] = {\n",
    "                \"Sector\": sector,\n",
    "                \"Year\": year,\n",
    "                **metrics\n",
    "            }\n",
    "            \n",
    "            print(f\"✅ Completed analysis for {name}\")\n",
    "        \n",
    "        await extractor.clean_cache()\n",
    "    \n",
    "    # Add the metrics to the original DataFrame\n",
    "    name_col = 'name' if 'name' in df.columns else 'name_x'\n",
    "    \n",
    "    for col in [\"Sector\", \"Year\", \"market_size\", \"cagr\", \"timing_score\", \"us_sentiment\",\n",
    "                \"sea_sentiment\", \"competitor_count\", \"total_funding\", \"momentum_score\", \"execution_time\"]:\n",
    "        df[col] = df[name_col].map(lambda name: company_cache.get(name, {}).get(col))\n",
    "    \n",
    "    # Save results\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"\\n🎉 Analysis complete! Results saved to {output_file}\")\n",
    "    print(f\"Total companies analyzed: {len(company_cache)}\")\n",
    "    print(f\"Total records with metrics: {len(df)}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# This function will be called in the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    print(\"🚀 Starting Complete Market Analysis Pipeline\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    try:\n",
    "        # Run the complete analysis\n",
    "        result_df = await run_analysis_and_save(\n",
    "            api_key=PERPLEXITY_API_KEY,\n",
    "            serper_key=SERPER_API_KEY,  # Updated serper key\n",
    "            input_file=\"profiles_data.csv\",\n",
    "            output_file=\"profiles_data_with_metrics.csv\"\n",
    "        )\n",
    "        \n",
    "        if result_df is not None:\n",
    "            print(\"\\n📊 Analysis Summary:\")\n",
    "            print(f\"Total records processed: {len(result_df)}\")\n",
    "            print(f\"Companies with metrics: {result_df['Sector'].notna().sum()}\")\n",
    "            \n",
    "            # Show sample of results\n",
    "            print(\"\\n📋 Sample Results:\")\n",
    "            sample_cols = ['name', 'Sector', 'market_size', 'cagr', 'timing_score', 'us_sentiment']\n",
    "            if 'name_x' in result_df.columns:\n",
    "                sample_cols[0] = 'name_x'\n",
    "            \n",
    "            available_cols = [col for col in sample_cols if col in result_df.columns]\n",
    "            print(result_df[available_cols].head(3).to_string(index=False))\n",
    "            \n",
    "            return result_df\n",
    "        else:\n",
    "            print(\"❌ Analysis failed\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error in main execution: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Execute the analysis\n",
    "final_results = await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def analyze_results(csv_file=\"profiles_data_with_metrics.csv\"):\n",
    "    \"\"\"Analyze and visualize the results\"\"\"\n",
    "    \n",
    "    if not os.path.isfile(csv_file):\n",
    "        print(f\"Results file {csv_file} not found. Please run the analysis first.\")\n",
    "        return\n",
    "    \n",
    "    # Load results\n",
    "    df = pd.read_csv(csv_file)\n",
    "    print(f\"Loaded {len(df)} records from {csv_file}\")\n",
    "    \n",
    "    # Filter out rows without metrics\n",
    "    df_with_metrics = df.dropna(subset=['Sector', 'market_size'])\n",
    "    print(f\"Records with complete metrics: {len(df_with_metrics)}\")\n",
    "    \n",
    "    if len(df_with_metrics) == 0:\n",
    "        print(\"No records with metrics found.\")\n",
    "        return\n",
    "    \n",
    "    # Set up the plotting style\n",
    "    plt.style.use('default')\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    fig.suptitle('Market Analysis Dashboard', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Market Size by Sector\n",
    "    sector_market = df_with_metrics.groupby('Sector')['market_size'].mean().sort_values(ascending=False)\n",
    "    axes[0, 0].bar(range(len(sector_market)), sector_market.values)\n",
    "    axes[0, 0].set_title('Average Market Size by Sector')\n",
    "    axes[0, 0].set_ylabel('Market Size (Billions USD)')\n",
    "    axes[0, 0].set_xticks(range(len(sector_market)))\n",
    "    axes[0, 0].set_xticklabels(sector_market.index, rotation=45, ha='right')\n",
    "    \n",
    "    # 2. CAGR Distribution\n",
    "    axes[0, 1].hist(df_with_metrics['cagr'], bins=10, alpha=0.7, color='skyblue')\n",
    "    axes[0, 1].set_title('CAGR Distribution')\n",
    "    axes[0, 1].set_xlabel('CAGR (%)')\n",
    "    axes[0, 1].set_ylabel('Frequency')\n",
    "    \n",
    "    # 3. Timing Score vs Market Size\n",
    "    axes[0, 2].scatter(df_with_metrics['timing_score'], df_with_metrics['market_size'], alpha=0.6)\n",
    "    axes[0, 2].set_title('Timing Score vs Market Size')\n",
    "    axes[0, 2].set_xlabel('Timing Score')\n",
    "    axes[0, 2].set_ylabel('Market Size (Billions USD)')\n",
    "    \n",
    "    # 4. Regional Sentiment Comparison\n",
    "    sentiment_data = df_with_metrics[['us_sentiment', 'sea_sentiment']].mean()\n",
    "    axes[1, 0].bar(['US Market', 'SEA Market'], sentiment_data.values, color=['blue', 'orange'])\n",
    "    axes[1, 0].set_title('Average Regional Sentiment')\n",
    "    axes[1, 0].set_ylabel('Sentiment Score')\n",
    "    axes[1, 0].set_ylim(0, 5)\n",
    "    \n",
    "    # 5. Competitor Count vs Funding\n",
    "    axes[1, 1].scatter(df_with_metrics['competitor_count'], df_with_metrics['total_funding'], alpha=0.6)\n",
    "    axes[1, 1].set_title('Competitor Count vs Total Funding')\n",
    "    axes[1, 1].set_xlabel('Number of Competitors')\n",
    "    axes[1, 1].set_ylabel('Total Funding (Billions USD)')\n",
    "    \n",
    "    # 6. Momentum Score Distribution\n",
    "    axes[1, 2].hist(df_with_metrics['momentum_score'], bins=10, alpha=0.7, color='lightgreen')\n",
    "    axes[1, 2].set_title('Market Momentum Score Distribution')\n",
    "    axes[1, 2].set_xlabel('Momentum Score')\n",
    "    axes[1, 2].set_ylabel('Frequency')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"\\n📊 SUMMARY STATISTICS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    numeric_cols = ['market_size', 'cagr', 'timing_score', 'us_sentiment', \n",
    "                   'sea_sentiment', 'competitor_count', 'total_funding', 'momentum_score']\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        if col in df_with_metrics.columns:\n",
    "            print(f\"\\n{col.upper().replace('_', ' ')}:\")\n",
    "            print(f\"  Mean: {df_with_metrics[col].mean():.2f}\")\n",
    "            print(f\"  Median: {df_with_metrics[col].median():.2f}\")\n",
    "            print(f\"  Std: {df_with_metrics[col].std():.2f}\")\n",
    "            print(f\"  Min: {df_with_metrics[col].min():.2f}\")\n",
    "            print(f\"  Max: {df_with_metrics[col].max():.2f}\")\n",
    "    \n",
    "    # Top performers\n",
    "    print(\"\\n🏆 TOP PERFORMERS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    if len(df_with_metrics) > 0:\n",
    "        name_col = 'name' if 'name' in df_with_metrics.columns else 'name_x'\n",
    "        \n",
    "        print(\"\\nTop 5 by Market Size:\")\n",
    "        top_market = df_with_metrics.nlargest(5, 'market_size')[name_col].tolist()\n",
    "        for i, company in enumerate(top_market, 1):\n",
    "            print(f\"  {i}. {company}\")\n",
    "        \n",
    "        print(\"\\nTop 5 by CAGR:\")\n",
    "        top_cagr = df_with_metrics.nlargest(5, 'cagr')[name_col].tolist()\n",
    "        for i, company in enumerate(top_cagr, 1):\n",
    "            print(f\"  {i}. {company}\")\n",
    "        \n",
    "        print(\"\\nTop 5 by Timing Score:\")\n",
    "        top_timing = df_with_metrics.nlargest(5, 'timing_score')[name_col].tolist()\n",
    "        for i, company in enumerate(top_timing, 1):\n",
    "            print(f\"  {i}. {company}\")\n",
    "\n",
    "# Run analysis (uncomment to execute)\n",
    "analyze_results()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
